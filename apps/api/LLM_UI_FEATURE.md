# LLM Settings UI Feature

## Overview

A comprehensive UI for managing LLM providers and models has been implemented, allowing users to dynamically switch between OpenAI and Anthropic models without restarting the server.

## âœ¨ What's New

### 1. LLM Settings Dialog Component

**Location:** `components/workspace/llm-settings.tsx`

A fully-featured settings dialog that provides:
- âœ… Provider selection (OpenAI vs Anthropic)
- âœ… Model selection with badges (Latest, Fast, Powerful)
- âœ… Secure API key input
- âœ… Real-time configuration updates
- âœ… Current configuration display
- âœ… Model information and descriptions
- âœ… Error handling and validation
- âœ… Success feedback

### 2. Latest Claude Models Support ğŸ†•

**Updated:** `src/lib/llm-config.ts`

Now includes the latest **Claude 4.x series** released in 2025:

#### Claude 4.x Models (2025)
- **`claude-sonnet-4-5`** (Default for Anthropic)
  - Released: September 29, 2025
  - Best for: Coding tasks and agentic workflows
  - Optimized for real-world applications

- **`claude-opus-4-1`**
  - Released: August 5, 2025
  - Best for: Complex reasoning and problem-solving
  - Most capable Claude model

- **`claude-haiku-4-5`**
  - Released: October 15, 2025
  - Best for: Real-time applications
  - Fastest, lowest latency

#### Also Supports
- All Claude 3.5 models (20241022, 20240620)
- All Claude 3 models (Opus, Sonnet, Haiku)

### 3. Runtime Configuration API

**Location:** `app/api/llm/config/route.ts`

RESTful API endpoints for LLM configuration management:

#### GET `/api/llm/config`
Returns current configuration and available models.

**Response:**
```json
{
  "provider": "anthropic",
  "model": "claude-sonnet-4-5",
  "availableModels": {
    "openai": ["gpt-4o", "gpt-4o-mini", ...],
    "anthropic": ["claude-sonnet-4-5", "claude-opus-4-1", ...]
  }
}
```

#### POST `/api/llm/config`
Updates configuration at runtime.

**Request:**
```json
{
  "provider": "anthropic",
  "model": "claude-sonnet-4-5",
  "apiKey": "sk-ant-..."
}
```

**Features:**
- âœ… Input validation (provider, model, API key format)
- âœ… Runtime configuration without server restart
- âœ… Environment variable fallback
- âœ… Secure API key handling

### 4. Enhanced Configuration Library

**Updated:** `src/lib/llm-config.ts`

**New Functions:**
- `setRuntimeLLMConfig(config)` - Set runtime configuration
- `clearRuntimeLLMConfig()` - Clear runtime and revert to env vars
- `getLLMConfig()` - Get current config (runtime > env > defaults)
- `getLLMModel()` - Get AI SDK model instance
- `getAvailableModels(provider)` - Get list of available models

**Configuration Priority:**
1. Runtime Config (set via UI)
2. Environment Variables
3. Default Values

### 5. Workspace Integration

**Updated:** `components/workspace/super-tabs.tsx`

The LLM Settings button is now prominently displayed in the workspace navigation bar, providing easy access to configuration.

## ğŸ¯ Features

### User Experience
- **No Server Restart**: Switch models on the fly
- **Visual Interface**: Dropdown selectors with model descriptions
- **Smart Defaults**: Recommended models are pre-selected
- **Model Badges**: Visual indicators (Latest, Fast, Powerful)
- **Validation**: Real-time validation of inputs
- **Feedback**: Clear success/error messages
- **Current Config Display**: See what's currently active

### Security
- **API Key Protection**: Never sent to client
- **Input Validation**: Provider, model, and API key format checks
- **Secure Storage**: Keys stored in memory/environment only
- **Format Validation**: 
  - OpenAI keys must start with `sk-`
  - Anthropic keys must start with `sk-ant-`

### Flexibility
- **Dual Configuration**: UI or environment variables
- **Runtime Updates**: Changes apply immediately
- **Backward Compatible**: Existing env-based configs still work
- **Fallback Support**: Graceful degradation if UI config fails

## ğŸ“ Files Changed

```
apps/api/
â”œâ”€â”€ components/workspace/
â”‚   â”œâ”€â”€ llm-settings.tsx          â† NEW: Settings dialog component
â”‚   â””â”€â”€ super-tabs.tsx             â† UPDATED: Added settings button
â”œâ”€â”€ app/api/llm/config/
â”‚   â””â”€â”€ route.ts                   â† NEW: Configuration API endpoints
â”œâ”€â”€ src/lib/
â”‚   â””â”€â”€ llm-config.ts              â† UPDATED: Runtime config + Claude 4.x
â””â”€â”€ LLM_PROVIDERS.md               â† UPDATED: Documentation
```

## ğŸš€ Usage

### For End Users

1. **Open Workspace**: Navigate to `/workspace`
2. **Click "LLM Settings"**: Button in top-right corner
3. **Configure**:
   - Select provider (OpenAI/Anthropic)
   - Choose model from dropdown
   - Enter API key (if changing providers)
4. **Save**: Configuration applies immediately!

### For Developers

#### Using Runtime Configuration
```typescript
import { setRuntimeLLMConfig } from "@/src/lib/llm-config";

// Set configuration programmatically
setRuntimeLLMConfig({
  provider: "anthropic",
  model: "claude-sonnet-4-5",
  apiKey: "sk-ant-..."
});
```

#### Using the Model
```typescript
import { getLLMModel } from "@/src/lib/llm-config";
import { generateText } from "ai";

// Automatically uses configured provider/model
const model = getLLMModel();

const { text } = await generateText({
  model,
  prompt: "Analyze this model card...",
});
```

## ğŸ¨ UI Components Used

- **Dialog**: Modal container from shadcn/ui
- **Select**: Dropdown selectors for provider/model
- **Input**: Password input for API keys
- **Button**: Action buttons
- **Badge**: Visual indicators for models
- **Alert**: Success/error messages
- **Label**: Form field labels

## ğŸ”§ Configuration Examples

### Example 1: Using Latest Claude Model
```typescript
// Via UI: Select "Anthropic" â†’ "Claude Sonnet 4.5"

// Via API:
fetch('/api/llm/config', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    provider: 'anthropic',
    model: 'claude-sonnet-4-5',
    apiKey: 'sk-ant-...'
  })
});
```

### Example 2: Using GPT-4o
```typescript
// Via UI: Select "OpenAI" â†’ "GPT-4o"

// Via API:
fetch('/api/llm/config', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    provider: 'openai',
    model: 'gpt-4o',
    apiKey: 'sk-...'
  })
});
```

## ğŸ“Š Model Comparison

### OpenAI Models

| Model | Speed | Cost | Use Case |
|-------|-------|------|----------|
| GPT-4o | Fast | High | Multimodal tasks |
| GPT-4o Mini | Fastest | Low | Cost-effective |
| GPT-4 Turbo | Medium | High | Complex tasks |

### Anthropic Models

| Model | Speed | Cost | Use Case |
|-------|-------|------|----------|
| Sonnet 4.5 | Fast | Medium | Coding & agents |
| Opus 4.1 | Slow | High | Complex reasoning |
| Haiku 4.5 | Fastest | Low | Real-time apps |

## ğŸ› Error Handling

The system provides clear error messages for:
- Invalid provider selection
- Missing models
- Incorrect API key format
- Network failures
- API errors

Example error messages:
```
âŒ Invalid OpenAI API key format. Should start with 'sk-'
âŒ Invalid provider. Must be 'openai' or 'anthropic'
âŒ Failed to save configuration: Network error
```

## ğŸ”’ Security Considerations

1. **API Keys**: Never logged or sent to client
2. **Validation**: All inputs validated server-side
3. **Environment Isolation**: Runtime config doesn't override .env file permanently
4. **HTTPS**: Always use HTTPS in production
5. **Rate Limiting**: Consider adding rate limits to config endpoint

## ğŸ“ Best Practices

### When to Use Each Model

**Claude Sonnet 4.5** (Recommended Default):
- âœ… Coding and debugging
- âœ… Agentic workflows
- âœ… Model card verification
- âœ… Complex tool use

**Claude Opus 4.1**:
- âœ… Deep reasoning tasks
- âœ… Complex analysis
- âœ… High-stakes decisions

**Claude Haiku 4.5**:
- âœ… Real-time assistants
- âœ… Customer support
- âœ… Quick responses

**GPT-4o**:
- âœ… Multimodal tasks
- âœ… Image analysis
- âœ… Vision + text

**GPT-4o Mini**:
- âœ… Cost-effective operations
- âœ… High-volume tasks
- âœ… Simple queries

## ğŸš¦ Migration Path

### From Environment Variables to UI

1. Keep your `.env` file as backup
2. Open LLM Settings in UI
3. Configure your preferred setup
4. Test thoroughly
5. Remove env vars if desired (optional)

### Reverting to Environment Variables

1. Delete runtime config via API or restart server
2. Ensure `.env` has correct values
3. Restart application

## ğŸ“ Notes

- Runtime config persists across requests but **not** server restarts
- For production, use environment variables for persistence
- API keys in runtime config take precedence over env vars
- UI configuration is stored in memory on the server
- No database storage is used (stateless)

## ğŸ”® Future Enhancements

Potential improvements:
- [ ] Database storage for persistent config
- [ ] User-specific configurations
- [ ] Model performance analytics
- [ ] Cost tracking per model
- [ ] A/B testing between models
- [ ] Model response comparison
- [ ] Automatic model selection based on task
- [ ] Token usage monitoring

## ğŸ‰ Summary

You now have a **fully functional LLM settings UI** that:
- âœ… Supports latest Claude 4.x models
- âœ… Allows runtime provider/model switching
- âœ… Provides a beautiful, intuitive interface
- âœ… Works without server restarts
- âœ… Maintains backward compatibility
- âœ… Handles errors gracefully
- âœ… Secures API keys properly

**Ready to use!** Just open the workspace and click "LLM Settings" in the top-right corner.


